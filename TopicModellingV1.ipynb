{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQU2gUS07nafvFTl4HASpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Padmini-RK/Yelp-Business-Data-Analysis/blob/main/TopicModellingV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pkalOS0uibSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78d086b-f625-4bfa-bc43-690062b197a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --ignore-installed pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02KEoDvhpQuP",
        "outputId": "47e7d960-05b7-46b2-ff9f-1b70db3d5176"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=647215d30bf9507e853b55164fc6b843b07672839996b88cedd1836c79dcdd18\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brYAQnLupvku",
        "outputId": "94c3a46f-c20e-491c-f721-8e5d1d82df98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.0\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 11.0.18\n",
            "Branch HEAD\n",
            "Compiled by user xinrong.meng on 2023-04-07T02:18:01Z\n",
            "Revision 87a5442f7ed96b11051d8a9333476d080054e5a0\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --ignore-installed spark-nlp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G046J3x4p7I_",
        "outputId": "a951c618-765b-460a-a53a-d4f9a7478421"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spark-nlp\n",
            "  Downloading spark_nlp-4.4.1-py2.py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.7/486.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-4.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN_2RQ32qKDJ",
        "outputId": "11ab54f5-888e-41a1-9137-e12f62c48752"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nltk --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2uHoFkiqUrk",
        "outputId": "f24fe11b-46ed-45bd-dce3-52df37437230"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk, version 3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "O93oqt28qohR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "path_to_review_data = '/content/drive/My Drive/ADBMS/review.json'\n",
        "review_data = spark.read.json(path_to_review_data)"
      ],
      "metadata": {
        "id": "8fUHEkZ_q22M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGvPplxcsRCh",
        "outputId": "457b2b05-5eb8-409e-d531-6c25f653ab55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['business_id',\n",
              " 'cool',\n",
              " 'date',\n",
              " 'funny',\n",
              " 'review_id',\n",
              " 'stars',\n",
              " 'text',\n",
              " 'useful',\n",
              " 'user_id']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_col_name = 'text'\n",
        "b_col = 'business_id'\n",
        "top_500 = spark.createDataFrame(review_data.head(20000))\n",
        "#review_text = review_data.select(text_col_name).filter(F.col(text_col_name).isNotNull())\n",
        "business_specific_review = top_500.select(text_col_name, 'business_id').filter(F.col(b_col) == \"GBTPC53ZrG1ZBY3DT8Mbcw\")\n",
        "\n",
        "#k = top_500.select(b_col).groupby(b_col).count().orderBy('count', ascending=False)\n",
        "#.show(20)\n",
        "#print(k.collect()[0][0])\n",
        "#print(k.collect()[0][1])\n"
      ],
      "metadata": {
        "id": "1GhK20J4srOa"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "business_specific_review.count()"
      ],
      "metadata": {
        "id": "fe9IT2JPxOPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9253136e-c149-4d73-fbfd-543b0f1604ab"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#review_text.limit(5).show(truncate=90)"
      ],
      "metadata": {
        "id": "fdVkUwkgs6ND"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(text_col_name) \\\n",
        "     .setOutputCol('document')"
      ],
      "metadata": {
        "id": "sBqT3IC2fjiQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['document']) \\\n",
        "     .setOutputCol('tokenized')\n",
        "     "
      ],
      "metadata": {
        "id": "oGz1WO_9iUO1"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Normalizer\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols(['tokenized']) \\\n",
        "     .setOutputCol('normalized') \\\n",
        "     .setLowercase(True)"
      ],
      "metadata": {
        "id": "4RjX1rzViZRB"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import LemmatizerModel\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols(['normalized']) \\\n",
        "     .setOutputCol('lemmatized')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBK_gZjhic8c",
        "outputId": "aa8ddd4c-586f-4390-bd2e-77c72ac46d3d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "eng_stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTatkht-ikxb",
        "outputId": "fc01a7f2-4b9b-4d48-b88b-791496d848a5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import StopWordsCleaner\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols(['lemmatized']) \\\n",
        "     .setOutputCol('unigrams') \\\n",
        "     .setStopWords(eng_stopwords)"
      ],
      "metadata": {
        "id": "AnbxdSLJiqqO"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import NGramGenerator\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['lemmatized']) \\\n",
        "    .setOutputCol('ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "metadata": {
        "id": "bBsqj4jMiuQp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import PerceptronModel\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
        "    .setInputCols(['document', 'lemmatized']) \\\n",
        "    .setOutputCol('pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBBbfhdoiylE",
        "outputId": "6b897fae-4197-40bb-f2b2-a4548bf4cb91"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import Finisher\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['unigrams', 'ngrams', 'pos']) "
      ],
      "metadata": {
        "id": "oIrxC_Fli6bw"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 pos_tagger,\n",
        "                 ngrammer,  \n",
        "                 finisher])"
      ],
      "metadata": {
        "id": "uVzTq5Y7jDFA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = pipeline.fit(business_specific_review).transform(business_specific_review)"
      ],
      "metadata": {
        "id": "MXKF_txqjGpW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr5o0ZKcjQ2Z",
        "outputId": "13ac5d3c-666d-4155-8ace-8ac7da2f8a49"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|         business_id|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|We have been here...|GBTPC53ZrG1ZBY3DT...|[twice, brunch, e...|[we, have, be, he...|[PRP, VBP, VB, RB...|\n",
            "|First meal in New...|GBTPC53ZrG1ZBY3DT...|[first, meal, new...|[first, meal, in,...|[JJ, NN, IN, JJ, ...|\n",
            "|Service is excell...|GBTPC53ZrG1ZBY3DT...|[service, excelle...|[service, be, exc...|[NN, VB, JJ, NN, ...|\n",
            "|This was our seco...|GBTPC53ZrG1ZBY3DT...|[second, choice, ...|[this, be, we, se...|[DT, VB, PRP, JJ,...|\n",
            "|Great oyster happ...|GBTPC53ZrG1ZBY3DT...|[great, oyster, h...|[great, oyster, h...|[JJ, NN, JJ, NN, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))\n",
        "     "
      ],
      "metadata": {
        "id": "s60mp4NTjlyX"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('finished_pos') \\\n",
        "     .setOutputCol('pos_document')"
      ],
      "metadata": {
        "id": "Exh2wVW2jqFi"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['pos_document']) \\\n",
        "     .setOutputCol('pos')"
      ],
      "metadata": {
        "id": "37R4Tef9jtEe"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['pos']) \\\n",
        "    .setOutputCol('pos_ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "metadata": {
        "id": "CrG1MbEpjwfS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_finisher = Finisher() \\\n",
        "     .setInputCols(['pos', 'pos_ngrams']) "
      ],
      "metadata": {
        "id": "7U1Hw0Zbj0um"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_pipeline = Pipeline() \\\n",
        "     .setStages([pos_documentAssembler,                  \n",
        "                 pos_tokenizer,\n",
        "                 pos_ngrammer,  \n",
        "                 pos_finisher])\n",
        "     "
      ],
      "metadata": {
        "id": "EfVjX07nj3_5"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
      ],
      "metadata": {
        "id": "3BHIzHk1j6zE"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfZmcPg-j9jN",
        "outputId": "6c0dbc53-5bd7-4c40-daf7-e3a2d236fdd1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'business_id',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hzZMXjtkDnt",
        "outputId": "f1a7923c-3431-429d-9fbc-43bc027922c9"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|     finished_ngrams| finished_pos_ngrams|\n",
            "+--------------------+--------------------+\n",
            "|[we, have, be, he...|[PRP, VBP, VB, RB...|\n",
            "|[first, meal, in,...|[JJ, NN, IN, JJ, ...|\n",
            "|[service, be, exc...|[NN, VB, JJ, NN, ...|\n",
            "|[this, be, we, se...|[DT, VB, PRP, JJ,...|\n",
            "|[great, oyster, h...|[JJ, NN, JJ, NN, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "6FO8rrb3kJWF"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = processed_review.withColumn('filtered_unigrams',\n",
        "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
        "                                                              F.col('finished_pos')))\n",
        "     "
      ],
      "metadata": {
        "id": "il9MVNlvkMOe"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ntfiNyWkP6a",
        "outputId": "da2b5fc0-1956-4ef3-edb1-73a4212e7ad4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                         filtered_unigrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|           [brunch, enjoy, delicious, grit, scallions, room, freeze, sure, bring, sweater]|\n",
            "|[first, meal, orleans, lunch, seafood, oyster, meat, claw, shrimp, come, surprisingly, ...|\n",
            "|[service, excellent, atmosphere, great, raw, bar, good, alaskan, crab, leg, well, splur...|\n",
            "|[choice, rio, mar, mardi, week, hell, time, side, parade, stay, great, service, overbea...|\n",
            "|[great, oyster, happy, hour, pm, dozen, glass, tip, value, hear, cup, totally, staff, n...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_pos_combs(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if (len(pos.split('_')) == 2 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
        "            or (len(pos.split('_')) == 3 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
        "    \n",
        "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))\n",
        "     "
      ],
      "metadata": {
        "id": "kmKkiVfzkaKl"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = processed_review.withColumn('filtered_ngrams',\n",
        "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
        "                                                                    F.col('finished_pos_ngrams')))\n",
        "     "
      ],
      "metadata": {
        "id": "9wDPJIcxkdWA"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3mFMFpjkgaT",
        "outputId": "4a399e15-1e71-426b-ef9f-ba6c8aac222d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                           filtered_ngrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|                                             [have_try, be_delicious, sour_cream, be_sure]|\n",
            "|[first_meal, new_orleans, lunch_special, seafood_stew, oyster_meat, meat_crab, crab_cla...|\n",
            "|[be_excellent, excellent_atmosphere, be_great, raw_bar, be_fantastic, good_alaskan, ala...|\n",
            "|[second_choice, rio_mar, mardi_gra, gra_week, other_side, stay_nonetheless, great_meal,...|\n",
            "|[great_oyster, oyster_happy, happy_hour, dozen_oyster, tip_great, great_value, good_tog...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat\n",
        "\n",
        "processed_review = processed_review.withColumn('final', \n",
        "                                               concat(F.col('filtered_unigrams'), \n",
        "                                                      F.col('filtered_ngrams')))\n",
        "     "
      ],
      "metadata": {
        "id": "Z2wSUdeHks-K"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('final').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzN_abOQkwfJ",
        "outputId": "8fe4b85f-ee61-4d7c-c168-c979d23117af"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                     final|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[brunch, enjoy, delicious, grit, scallions, room, freeze, sure, bring, sweater, have_tr...|\n",
            "|[first, meal, orleans, lunch, seafood, oyster, meat, claw, shrimp, come, surprisingly, ...|\n",
            "|[service, excellent, atmosphere, great, raw, bar, good, alaskan, crab, leg, well, splur...|\n",
            "|[choice, rio, mar, mardi, week, hell, time, side, parade, stay, great, service, overbea...|\n",
            "|[great, oyster, happy, hour, pm, dozen, glass, tip, value, hear, cup, totally, staff, n...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ],
      "metadata": {
        "id": "KQb7-OAnk_ak"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ],
      "metadata": {
        "id": "0p8CVBry1byI"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 6\n",
        "max_iter = 10\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)\n",
        "     "
      ],
      "metadata": {
        "id": "QWc4ZBb41o9B"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
        "     "
      ],
      "metadata": {
        "id": "cA-PLqC016QB"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_words = 10\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv20Bucd2IIE",
        "outputId": "832324f1-3e70-47a7-ef89-941b5f38d28e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|topic|                                                                                topicWords|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|    0|      [even, meat, solid, make, great_service, casual, new_orleans, fare, orleans, always]|\n",
            "|    1|  [oyster, fry_oyster, fry, good, oyster_salad, take, salad, get, enjoy, fry_oyster_salad]|\n",
            "|    2|                   [however, lüke, come, second, night, fave, quickly, see, be_worth, bar]|\n",
            "|    3|        [shrimp, wild_mushroom, thin, waiter, wild, hard, tender, mushroom, raw_bar, pork]|\n",
            "|    4|                 [order, try, burger, be_great, beer, crab, be_perfect, great, beet, also]|\n",
            "|    5|[coffee, happy, amazing, customer_service, turn, adequate, meatlovers, drink_make, besh...|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topicdf = topics.select('topicWords')\n",
        "topicdf.collect()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqnW13Ke4mij",
        "outputId": "b3397090-7861-4d81-9e86-8c477b42f10e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['holiday_special',\n",
              " 'unable',\n",
              " 'imaginethe_service',\n",
              " 'couple_friend',\n",
              " 'thing',\n",
              " 'thinly',\n",
              " 'single_color',\n",
              " 'appt_im',\n",
              " 'okay',\n",
              " 'build_vietnamese']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}